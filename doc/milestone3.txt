Julian on object detection and mapping:
At Milestone 2 the 2D image data was filtered by HSV colour values and modified to get the opencv method findcontours to work. Since then findcontours was expanded upon to function as a way of object segmentation since the output is a list of sequences each corresponding to a single connected contour. Using the bounding boxes of the individual contours max/min x/y values we determine the central pixel for a primitive form of object detection. The point of the pointcloud corresponding to that center-pixel is published with it's colour value as a detected and identified object. An alternative approach where not just the central point but the whole segmented object (determined by colour and regions of the binary image used in findcontours) was returned as a pointcloud2 message was implemented but is not used (yet).
We also spent a lot of time trying to get the python bindings for PCL to work and ultimately didn't use them since apparently the methods for conversion to and from ROS messages are not implemented there. There would have been a way of returning the whole pointcloud as a list and transforming that into a PCL-compatible format but we made do without PCL so far.

I also started on implementing an occupancy grid in map_node.py while referencing https://github.com/RiccardoGiubilato/ros_autonomous_car/blob/master/src/laser_to_occupancy_grid.py but ran out of time, a nav_msgs.msg OccupancyGrid message is published but the transformations are not used correctly at the time of the milestone meeting.
